{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Import\n",
    "!pip install python-dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from bertopic import BERTopic\n",
    "import random\n",
    "import math\n",
    "import configparser\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append('')\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Initial Setup\n",
    "\n",
    "## Set up the LLaMA Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "local_model_path = './Llama'  # Update if different\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "llama_model = llama_model.to(device)\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "## Initial Settings (update config.ini file)\n",
    "result_path = r'C:\\Users\\ppaloju\\OneDrive - MetLife\\Documents\\MetIQ_result.xlsx'\n",
    "data_file_path = r'C:\\Users\\ppaloju\\OneDrive - MetLife\\Documents\\RMfile.xlsx'\n",
    "text_column = 'interaction'\n",
    "num_sample_data = 200\n",
    "random_seed = 42\n",
    "test_size = 100\n",
    "\n",
    "ini_exampled_topic_set = {\"Termination\", \"Bill Generation & Payment related\", \"Action Taken and Resolutions\"}\n",
    "Document_1 = \"BA had retro terminations Terminations Processed\"\n",
    "topic_1 = \"Termination\"\n",
    "Document_2 = \"Billing issue No action required, the CSC placed a call with the group & provided a bill vs paid report for review and advise if any additional research is needed.\"\n",
    "topic_2 = \"Bill Generation & Payment related\"\n",
    "Document_3 = \"Group was terminated in then instated and the ee are still showing terminated in GF\"\n",
    "topic_3 = \"Action Taken and Resolutions\"\n",
    "Document_4 = \"\"\n",
    "topic_4 = \"\"\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./config.ini')\n",
    "result_path = config.get('dataset', 'result_path')\n",
    "data_file_path = config.get('dataset', 'data_file_path')\n",
    "text_column = config.get('dataset', 'text_column')\n",
    "num_sample_data = config.getint('dataset', 'num_sample_data')\n",
    "random_seed = config.getint('dataset', 'random_seed')\n",
    "test_size = config.getint('dataset', 'test_size')\n",
    "use_examples = config.getboolean('Domain example', 'use_examples', fallback=True)\n",
    "ini_exampled_topic_set = config.get('Domain example', 'ini_exampled_topic_set')\n",
    "ini_exampled_topic_set = ast.literal_eval(ini_exampled_topic_set)\n",
    "Document_1 = config.get('Domain example', 'Document_1')\n",
    "topic_1 = config.get('Domain example', 'topic_1')\n",
    "Document_2 = config.get('Domain example', 'Document_2')\n",
    "topic_2 = config.get('Domain example', 'topic_2')\n",
    "Document_3 = config.get('Domain example', 'Document_3')\n",
    "topic_3 = config.get('Domain example', 'topic_3')\n",
    "Document_4 = config.get('Domain example', 'Document_4')\n",
    "topic_4 = config.get('Domain example', 'topic_4')\n",
    "\n",
    "## Load the Data\n",
    "raw_df = pd.read_excel(data_file_path)\n",
    "raw_df[text_column] = raw_df['COMMENTS'] + ' ' + raw_df['ACTIONS']\n",
    "raw_df[text_column] = raw_df[text_column].fillna(\"\").astype(str)\n",
    "\n",
    "## Preprocess for Call Center Transcripts (Optional)\n",
    "def preprocess_transcript(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\b(um|uh|like|you know|hello|hi|good morning|thank you|bye)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'^(Agent|Customer)\\s*:\\s*', '', text, flags=re.MULTILINE)\n",
    "    return ' '.join(text.split()).strip()\n",
    "\n",
    "def segment_transcript(text, max_length=500):\n",
    "    if not text:\n",
    "        return []\n",
    "    sentences = text.split('.')\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length > max_length:\n",
    "            segments.append('. '.join(current_segment) + '.')\n",
    "            current_segment = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_segment.append(sentence)\n",
    "            current_length += sentence_length\n",
    "    if current_segment:\n",
    "        segments.append('. '.join(current_segment) + '.')\n",
    "    return segments\n",
    "\n",
    "# Apply preprocessing (uncomment for transcripts)\n",
    "# raw_df[text_column] = raw_df[text_column].apply(preprocess_transcript)\n",
    "# raw_df['segments'] = raw_df[text_column].apply(segment_transcript)\n",
    "# raw_df = raw_df.explode('segments').reset_index(drop=True)\n",
    "# raw_df = raw_df.rename(columns={'segments': text_column})\n",
    "# raw_df = raw_df[raw_df[text_column].notna() & (raw_df[text_column] != '')]\n",
    "  \n",
    "\n",
    "## BERTopic for Sampling\n",
    "emb_model = SentenceTransformer('./model/all-mpnet-base-v2')\n",
    "topic_model = BERTopic(embedding_model=emb_model, min_topic_size=3)\n",
    "cleaned_text = raw_df[text_column].str.strip()\n",
    "cleaned_text = cleaned_text[cleaned_text != \"\"]\n",
    "documents = cleaned_text.str.lower().drop_duplicates()\n",
    "topics, probs = topic_model.fit_transform(documents)\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_representations = {topic_id: \", \".join([word for word, _ in topic_model.get_topic(topic_id)]) for topic_id in topic_info[\"Topic\"]}\n",
    "doc_topic_df = pd.DataFrame({\"doc_id\": range(len(documents)), \"text\": documents, \"topic\": topics})\n",
    "valid_topics = [t for t in topic_representations.keys() if t != -1]\n",
    "\n",
    "if len(valid_topics) > num_sample_data:\n",
    "    sample_per_topic = 1\n",
    "else:\n",
    "    sample_per_topic = math.ceil(num_sample_data / len(valid_topics))\n",
    "\n",
    "selected_samples = []\n",
    "random.seed(random_seed)\n",
    "for topic_id in valid_topics:\n",
    "    topic_docs = doc_topic_df[doc_topic_df[\"topic\"] == topic_id]\n",
    "    num_docs_in_topic = len(topic_docs)\n",
    "    num_to_sample = min(sample_per_topic, num_docs_in_topic)\n",
    "    sampled_docs = topic_docs.sample(num_to_sample, random_state=random_seed) if num_docs_in_topic > 0 else pd.DataFrame()\n",
    "    for _, row in sampled_docs.iterrows():\n",
    "        selected_samples.append((topic_id, row[\"text\"], topic_representations[topic_id]))\n",
    "sampled_df = pd.DataFrame(selected_samples, columns=[\"topic\", \"sampled_text\", \"key_words_representation\"])\n",
    "\n",
    "def create_genai_input(df):\n",
    "    merged_df = df.groupby([\"topic\", \"key_words_representation\"])[\"sampled_text\"].apply(lambda x: \"\\n***************************\\n\".join(x)).reset_index()\n",
    "    merged_df[\"merged_text\"] = merged_df[\"sampled_text\"] + \"\\n\\n[key words representation]\\n\" + merged_df[\"key_words_representation\"]\n",
    "    return merged_df[[\"merged_text\"]]\n",
    "\n",
    "genai_input_df = create_genai_input(sampled_df)\n",
    "\n",
    "# Step 1: Topic Generation\n",
    "example_document_list = []\n",
    "example_topic_list = []\n",
    "for i in range(1, 5):\n",
    "    document = globals()[f\"Document_{i}\"]\n",
    "    topic = globals()[f\"topic_{i}\"]\n",
    "    if document != \"\" and topic != \"\":\n",
    "        example_document_list.append(document)\n",
    "        example_topic_list.append(topic)\n",
    "\n",
    "def prepare_example(topic_set, example_documents, example_topic_list):\n",
    "    if not use_examples:\n",
    "        return \"\"\n",
    "    result = \"[Examples]\\nExampled topic list: {}\\n-----------------\\n\".format(list(topic_set))\n",
    "    for i in range(len(example_documents)):\n",
    "        if example_topic_list[i] not in topic_set:\n",
    "            result += \"Example: Generating a new topic: \\\"{}\\\"\\nDocument:\\n{}\\nYour response:\\n{}\\n--------------------\\n\".format(\n",
    "                example_topic_list[i], example_documents[i], example_topic_list[i])\n",
    "        else:\n",
    "            result += \"Example: Using an existing topic: \\\"{}\\\" (only if no better new topic can be generated).\\nDocument:\\n{}\\nYour response:\\n{}\\n--------------------\\n\".format(\n",
    "                example_topic_list[i], example_documents[i], example_topic_list[i])\n",
    "    return result\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "You will receive single/multiple documents and a set of topics. Your task is to identify topics within the document. \n",
    "Your priority is to generate NEW topics unless an existing topic in the provided topic list PERFECTLY captures the documents' essence.\n",
    "\n",
    "[TOPICS]\n",
    "{topic_list}\n",
    "{examples}\n",
    "\n",
    "[Instructions]\n",
    "Step 1: Prioritize creating new topics whenever possible:\n",
    "- Always attempt to generate a NOVEL topic (2-5 words) unless an exact match exists in the topic list.\n",
    "- Avoid generic terms (e.g., \"Issues\", \"Problems\") or single letters.\n",
    "- Ensure topics are DIVERSE and cover DISTINCT aspects (e.g., billing, terminations, resolutions).\n",
    "- Avoid multiple topics for the same theme (e.g., do not generate \"Billing Issues\" and \"Billing Problems\").\n",
    "- Return AT MOST 1-2 topics per document.\n",
    "- Use the [key words representation] at the end of each document list for reference.\n",
    "\n",
    "Step 2: Identify general topic(s) in the document:\n",
    "- If a single dominant topic is identified, return that topic.\n",
    "- If multiple strong topics emerge, return up to 2 topics separated by a comma.\n",
    "- Strive for GENERIC and UNIQUE topics that represent all given documents.\n",
    "\n",
    "[Document]\n",
    "{document}\n",
    "\n",
    "[Your response]\n",
    "Return ONLY the relevant topics without any additional text or prefixes.\n",
    "Example: Unacceptable: \"Issues\", \"Billing Problems\"\n",
    "Document: The billing was incorrect.\n",
    "Your response: Billing Errors\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = prompt1.format(examples=prepare_example(ini_exampled_topic_set, example_document_list, example_topic_list))\n",
    "\n",
    "prompt1_copy = prompt1[:]\n",
    "topic_set_copy = ini_exampled_topic_set.copy()\n",
    "\n",
    "def get_response(row):\n",
    "    global topic_set_copy\n",
    "    updated_topic_list = list(topic_set_copy.copy())\n",
    "    exe_prompt1 = prompt1_copy.format(topic_list='\\n'.join(updated_topic_list), document=row[\"merged_text\"])\n",
    "    inputs = llama_tokenizer(exe_prompt1, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            top_p=0.85,\n",
    "            temperature=0.5,\n",
    "            pad_token_id=llama_tokenizer.eos_token_id,\n",
    "            eos_token_id=llama_tokenizer.eos_token_id\n",
    "        )\n",
    "    result = llama_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    if \"[Your response]\" in result:\n",
    "        result = result.split(\"[Your response]\")[-1].strip()\n",
    "    topics = [t.strip() for t in result.split(',') if t.strip()]\n",
    "    cleaned_topics = [t for t in topics if t.lower() not in [\"general comment\", \"none\", \"\", \"issues\", \"problems\"]]\n",
    "    topic_set_copy.update(cleaned_topics)\n",
    "    return ', '.join(cleaned_topics)\n",
    "\n",
    "def apply_with_progress(df, func):\n",
    "    tqdm.pandas()\n",
    "    return df.progress_apply(func, axis=1)\n",
    "\n",
    "genai_input_df[\"Llama_Predicted_Topic\"] = apply_with_progress(genai_input_df, get_response)\n",
    "\n",
    "topic_list = []\n",
    "for item in genai_input_df[\"Llama_Predicted_Topic\"]:\n",
    "    if pd.notna(item) and item.strip():\n",
    "        topics = [t.strip() for t in item.split(',') if t.strip()]\n",
    "        topic_list.extend(topics)\n",
    "\n",
    "clean_topic_list = [re.sub(r'[^A-Za-z0-9 ]+', '', x).strip().lower().capitalize() for x in topic_list]\n",
    "clean_topic_list = list(set(clean_topic_list))\n",
    "\n",
    "def clean_topics(topics):\n",
    "    filtered_list = [t for t in topics if not (\n",
    "        len(t.strip()) <= 2 or\n",
    "        t.lower() in ['issues', 'problems', 'general', 'none']\n",
    "    )]\n",
    "    emb_model = SentenceTransformer('./model/all-mpnet-base-v2')\n",
    "    embeddings = emb_model.encode(filtered_list)\n",
    "    cleaned_list = []\n",
    "    seen = set()\n",
    "    for i, topic in enumerate(filtered_list):\n",
    "        if topic.lower() in seen:\n",
    "            continue\n",
    "        topic_emb = embeddings[i]\n",
    "        is_unique = True\n",
    "        for seen_topic, seen_emb in cleaned_list:\n",
    "            if cosine_similarity([topic_emb], [seen_emb])[0][0] > 0.8:\n",
    "                is_unique = False\n",
    "                break\n",
    "        if is_unique:\n",
    "            cleaned_list.append((topic, topic_emb))\n",
    "            seen.add(topic.lower())\n",
    "    return [t for t, _ in cleaned_list]\n",
    "\n",
    "clean_topic_list = clean_topics(clean_topic_list)\n",
    "print(\"Clean topic list after Step 1:\", clean_topic_list)\n",
    "\n",
    "# Step 2: Topic Refinement\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def cluster_topics(topics):\n",
    "    if not topics:\n",
    "        return []\n",
    "    emb_model = SentenceTransformer('./model/all-mpnet-base-v2')\n",
    "    embeddings = emb_model.encode(topics)\n",
    "    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.1, linkage='average')\n",
    "    labels = clustering.fit_predict(embeddings)\n",
    "    clustered_topics = {}\n",
    "    for topic, label in zip(topics, labels):\n",
    "        if label not in clustered_topics:\n",
    "            clustered_topics[label] = []\n",
    "        clustered_topics[label].append(topic)\n",
    "    return [max(cluster, key=len) for cluster in clustered_topics.values()]\n",
    "\n",
    "clean_topic_list = cluster_topics(clean_topic_list)\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "You are an expert in topic clustering for insurance customer feedback. Your task is to analyze the provided list of topics and create EXACTLY 10 DISTINCT and MEANINGFUL categories.\n",
    "\n",
    "[Instructions]:\n",
    "1. Create EXACTLY 10 categories that are COMPLETELY DISTINCT - each must represent a different aspect of feedback.\n",
    "2. DO NOT use variations of the same concept (e.g., avoid \"Billing Issues\" and \"Billing Problems\").\n",
    "3. Consolidate similar topics into a single category (e.g., merge all billing-related topics into \"Billing & Payments\").\n",
    "4. Use specific, descriptive names (2-5 words) relevant to insurance feedback.\n",
    "5. Ensure categories cover ALL major feedback aspects (e.g., billing, claims, terminations, customer support).\n",
    "6. DO NOT use generic placeholders (e.g., \"Category1\").\n",
    "7. Return ONLY a Python list with EXACTLY 10 unique categories.\n",
    "\n",
    "[Examples]:\n",
    "- Unacceptable: ['Billing Issues', 'Billing Problems', 'Payment Issues']\n",
    "- Acceptable: ['Billing & Payments', 'Claims Processing', 'Termination Issues', 'Customer Support']\n",
    "\n",
    "[Input list]\n",
    "{topics}\n",
    "\n",
    "[Your response]\n",
    "['Billing & Payments', 'Claims Processing', 'Termination Issues', 'Customer Support', 'Account Management', 'Enrollment Process', 'Technical Support', 'Plan Coverage', 'Policy Clarity', 'Communication Issues']\n",
    "\"\"\"\n",
    "\n",
    "inputs = llama_tokenizer(prompt2.format(topics=\"\\n\".join(clean_topic_list)), return_tensors=\"pt\", truncation=True).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = llama_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        temperature=0.1,\n",
    "        pad_token_id=llama_tokenizer.eos_token_id,\n",
    "        eos_token_id=llama_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "result = llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "if \"[Your response]\" in result:\n",
    "    response_text = result.split(\"[Your response]\")[-1].strip()\n",
    "else:\n",
    "    response_text = result.strip()\n",
    "\n",
    "try:\n",
    "    refined_clean_topic_list = ast.literal_eval(response_text)\n",
    "    refined_clean_topic_list = list(dict.fromkeys(t for t in refined_clean_topic_list if not re.match(r'Category\\d+', t)))\n",
    "    if len(refined_clean_topic_list) < 10:\n",
    "        topic_counts = Counter(clean_topic_list)\n",
    "        top_topics = [t for t, _ in topic_counts.most_common(10 - len(refined_clean_topic_list))]\n",
    "        refined_clean_topic_list.extend(t for t in top_topics if t not in refined_clean_topic_list)\n",
    "    refined_clean_topic_list = refined_clean_topic_list[:10]\n",
    "except:\n",
    "    matches = re.findall(r\"['\\\"]([^'\\\"]+?)['\\\"]\", response_text)\n",
    "    refined_clean_topic_list = list(dict.fromkeys(m for m in matches if not re.match(r'Category\\d+', m)))\n",
    "    if len(refined_clean_topic_list) < 10:\n",
    "        topic_counts = Counter(clean_topic_list)\n",
    "        top_topics = [t for t, _ in topic_counts.most_common(10 - len(refined_clean_topic_list))]\n",
    "        refined_clean_topic_list.extend(t for t in top_topics if t not in refined_clean_topic_list)\n",
    "    refined_clean_topic_list = refined_clean_topic_list[:10]\n",
    "\n",
    "if len(set(refined_clean_topic_list)) != 10:\n",
    "    raise ValueError(\"Step 2 did not produce exactly 10 unique categories\")\n",
    "\n",
    "print(\"Refined topic list after Step 2:\", refined_clean_topic_list)\n",
    "\n",
    "# Step 3: Topic Assignment\n",
    "prompt3 = \"\"\"\n",
    "You will receive a document and a predefined topic list. Your task is to assign the document to the most relevant topic from the list based on the content.\n",
    "\n",
    "[Instructions]\n",
    "1. Only use topics from the provided list - DO NOT create new topics.\n",
    "2. Return ONLY the topic selected from the list, without any additional text or prefix.\n",
    "3. For multiple related topics, return the most dominant topic.\n",
    "4. If the document does not relate to any topic, return \"No topic\".\n",
    "\n",
    "Topic List:\n",
    "{topic_list}\n",
    "\n",
    "[Document]:\n",
    "{document}\n",
    "\n",
    "[Your response]\n",
    "\"\"\"\n",
    "\n",
    "prompt3 = prompt3.format(topic_list=\", \".join(refined_clean_topic_list))\n",
    "\n",
    "def get_response_step3(row):\n",
    "    exe_prompt3 = prompt3.format(document=row[text_column])\n",
    "    inputs = llama_tokenizer(exe_prompt3, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=40,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=llama_tokenizer.eos_token_id,\n",
    "            eos_token_id=llama_tokenizer.eos_token_id\n",
    "        )\n",
    "    result = llama_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    if \"[Your response]\" in result:\n",
    "        result = result.split(\"[Your response]\")[-1].strip()\n",
    "    result = re.sub(r'\\[.*?\\]', '', result).strip()\n",
    "    return result if result in refined_clean_topic_list else \"No topic\"\n",
    "\n",
    "raw_df[\"Llama_Assigned_Topic\"] = apply_with_progress(raw_df, get_response_step3)\n",
    "raw_df.to_excel(result_path, index=False)\n",
    "\n",
    "# Validation\n",
    "print(\"Assigned topic distribution:\", raw_df['Llama_Assigned_Topic'].value_counts())\n",
    "sample_results = raw_df[[text_column, 'Llama_Assigned_Topic']].sample(5)\n",
    "for _, row in sample_results.iterrows():\n",
    "    print(f\"Document: {row[text_column][:100]}...\\nAssigned Topic: {row['Llama_Assigned_Topic']}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
